{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Classification Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom langfair.metrics.classification import ClassificationMetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n\nLarge language models (LLMs) used in classification use cases should be assessed for group fairness (if applicable). Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms.  LangFair offers the following classification fairness metrics from the LLM fairness literature:\n\n* Predicted Prevalence Rate Disparity [Feldman et al., 2015](https://arxiv.org/abs/1412.3756) , [Bellamy et al., 2018](https://arxiv.org/abs/1810.01943) , [Saleiro et al., 2019](https://arxiv.org/abs/1811.05577)\n* False Negative Rate Disparity [Bellamy et al., 2018](https://arxiv.org/abs/1810.01943) , [Saleiro et al., 2019](https://arxiv.org/abs/1811.05577)\n* False Omission Rate Disparity [Bellamy et al., 2018](https://arxiv.org/abs/1810.01943) , [Saleiro et al., 2019](https://arxiv.org/abs/1811.05577)\n* False Positive Rate Disparity [Bellamy et al., 2018](https://arxiv.org/abs/1810.01943) , [Saleiro et al., 2019](https://arxiv.org/abs/1811.05577)\n* False Discovery Rate Disparity [Bellamy et al., 2018](https://arxiv.org/abs/1810.01943) , [Saleiro et al., 2019](https://arxiv.org/abs/1811.05577)\n2. Assessment\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Simulate dataset for this example. In practice, users should replace this data with predicted classes generated by the LLM,\n# corresponding ground truth values, and corresponding protected attribute group data.\nsample_size = 10000\ngroups = np.random.binomial(n=1, p=0.5, size=sample_size)\ny_pred = np.random.binomial(n=1, p=0.3, size=sample_size)\ny_true = np.random.binomial(n=1, p=0.3, size=sample_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Metrics\n``ClassificationMetrics()`` - For calculating FaiRLLM (Fairness of Recommendation via LLM) metrics (class)\n\n**Class parameters:**\n\n  - ``metric_type`` - (**{'all', 'assistive', 'punitive', 'representation'}, default='all'**) Specifies which metrics to use.\n\n**Methods:**\n\n1. ``evaluate`` - Returns min, max, range, and standard deviation of metrics across protected attribute groups.\n\n  **Method Parameters:**\n    - ``groups`` - (**array-like**) Group indicators. Must contain exactly two unique values.\n    - ``y_pred`` - (**array-like**) Binary model predictions. Positive and negative predictions must be 1 and 0, respectively.\n    - ``y_true`` - (**array-like**) Binary labels (ground truth values). Positive and negative labels must be 1 and 0, respectively.\n    - ``ratio`` - (**boolean**) Indicates whether to compute the metric as a difference or a ratio\n\n    Returns:\n    - Dictionary containing fairness metric values (**Dictionary**).\n\nGenerate an instance of class ``ClassificationMetrics`` using default ``metric_type='all'``, which includes \"assistive\", \"punitive\", and \"representation\" metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cm = ClassificationMetrics(metric_type=\"all\")\n\n# Metrics expressed as ratios (target value of 1)\ncm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=True)\n\n# Metrics expressed as differences (target value of 0)\ncm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}