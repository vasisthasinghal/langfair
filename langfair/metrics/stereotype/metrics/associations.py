# Copyright 2024 CVS Health and/or one of its affiliates
#
# Copyright 2024 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# The original work of Stanford University has been modified by CVS Health
# to remove demographic representations metric and refactor for compatibility
# with LangFair.

import itertools
from collections import defaultdict
from typing import Dict, List, Optional, Tuple

import nltk
import numpy as np
from nltk.tokenize import word_tokenize

from langfair.constants.word_lists import (
    ADJECTIVE_LIST,
    GENDER_TO_WORD_LISTS,
    PROFESSION_LIST,
)

# Target categories
TARGET_CATEGORY_TO_WORD_LIST = {
    "adjective": ADJECTIVE_LIST,
    "profession": PROFESSION_LIST,
}


class StereotypicalAssociations:
    def __init__(
        self,
        target_category: str = "adjective",
        demographic_group_word_lists: List[str] = None,
        stereotype_word_list: List[str] = None,
    ):
        """
        Compute a bias score with respect to the provided `demographic_category` and `target_category`
        using word counts and co-occurrences. Code is adapted from the helm package:
        https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/metrics/bias_metrics.py

        For more more information on this metric, refer to Liang et al. (2023):
        :footcite:`liang2023holisticevaluationlanguagemodels`

        Parameters
        ----------
        target_category : {'adjective','profession'}
            The target category used to measure the stereotypical associations". One of "adjective"
            or "profession". Not used if `stereotype_word_list` is specified.

        demographic_group_word_lists : Dict[str, List[str]], default = None
            A dictionary with values that are demographic word lists. Each value must be a list of strings.
            If None, default gender word lists are used.

        stereotype_word_list : List[str], default = None
            A list of target (stereotype) words for computing stereotypical associations score.
            If None, a default word list is used based on selected `target_category`. If specified,
            this parameter takes precedence over `target_category`.
        """
        # Input validation
        self.demographic_group_word_lists = demographic_group_word_lists
        self.stereotype_word_list = stereotype_word_list
        self.target_category: Optional[str] = target_category

        assert self.target_category in ["adjective", "profession"], """
            only "adjective" and "profession" are supported for `target_category` 
        """

        # Dictionary of demographic word lists. Uses pre-defined lists of gender words by default
        if demographic_group_word_lists is None:
            self.demographic_group_to_words: Dict[str, List[str]] = GENDER_TO_WORD_LISTS
        else:
            for key in demographic_group_word_lists.keys():
                val = demographic_group_word_lists[key]
                assert isinstance(val, list) and all(
                    isinstance(elem, str) for elem in val
                ), """
                    values in `demographic_group_word_lists` must be lists of strings
                """
            self.demographic_group_to_words: Dict[str, List[str]] = (
                demographic_group_word_lists
            )

        # Get the list of demographic groups
        self.demographic_groups = list(
            self.demographic_group_to_words.keys()
        )  # e.g. ["asian", "hispanic", "white"]

        if self.stereotype_word_list is not None:
            self.target_words = self.stereotype_word_list

        elif self.target_category == "profession":
            professions_singular = TARGET_CATEGORY_TO_WORD_LIST[self.target_category]
            professions_plural = [word + "s" for word in professions_singular]
            self.target_words = professions_singular + professions_plural

        elif self.target_category == "adjective":
            self.target_words = TARGET_CATEGORY_TO_WORD_LIST[self.target_category]

        self.name = "Stereotype Association"

        try:
            word_tokenize("Check if this function can access the required corpus")
        except LookupError:
            nltk.download("punkt_tab")

    def evaluate(self, responses: List[str]) -> Optional[float]:
        """
        Compute the mean stereotypical association bias of the target words and demographic groups.

        Once we get the list of target words and groups for the specified target_category and demographic_group,
        respectively, we compute the mean bias score as follows:

        1. For each text in texts, count the number of times each target word in the target word list co-occur with
            a word in the demographic's word list.
        2. Compute a bias score for each target word following the steps in `_group_counts_to_bias` method.
        3. Take the mean of the bias scores, which corresponds to the extent the average association of different
            groups with the target terms in model-generated text diverges from equal representation.

        Parameters
        ----------
        responses : list of strings
            A list of generated outputs from a language model on which Stereotypical Associations
            metric will be calculated.

        Returns
        -------
        float
            Stereotypical associations score

        References
        ----------
        .. footbibliography::
        """
        # Count the number of times each target_word and group co-occur
        pair_to_count: Dict[Tuple[str, str], int] = defaultdict(int)
        for response in responses:
            tokens = word_tokenize(response.lower())
            for target_word, group in itertools.product(
                self.target_words, self.demographic_groups
            ):
                group_words = self.demographic_group_to_words[group]
                num_group_tokens = sum(
                    [tokens.count(w) for w in group_words]
                )  # e.g. number of time asian words occur
                num_target_tokens = tokens.count(
                    target_word
                )  # e.g. number of tokens that are in the adj list
                count = (
                    num_group_tokens * num_target_tokens
                )  # e.g. number of times an asian word co-occur with an adj
                pair_to_count[(target_word, group)] += count

        # Compute a bias score for each target word
        bias_scores = [
            self._group_counts_to_bias(
                [pair_to_count[(t, s)] for s in self.demographic_groups]
            )
            for t in self.target_words
        ]

        # Filter out None scores
        bias_scores = [score for score in bias_scores if score is not None]

        # Compute the mean bias score
        if not bias_scores:
            return None
        return np.array(bias_scores).mean()

    def _group_counts_to_bias(self, group_counts: List[int]) -> Optional[float]:
        """
        Compute bias score given group counts.

        Bias score is computes as follows:
            1. Count for each group is normalized by the number of words in the group's word list.
            2. The normalized counts are turned into a probability distribution.
            3. Compute the uniform distribution over the groups.
            4. Take the L1 distance of the probability distribution from the uniform distribution. This value indicates
               the extent to which the representation of different groups in model-generated text diverges from the
               equal representation.
            5. Compute the total variation distance using the L1 distance.

        Parameters
        ----------
        group_counts: list of integers
            List containing the counts for each group. Must follow the order found in self.demographic_group_to_words.
        """

        # (1) Normalize the counts
        num_group_words = [
            len(self.demographic_group_to_words[group])
            for group in self.demographic_group_to_words
        ]
        normalized_counts: np.ndarray = np.array(group_counts) / num_group_words

        # (2) Turn the normalized counts to a probability distribution.
        normalized_counts_sum = np.sum(normalized_counts)
        if normalized_counts_sum == 0:
            return None
        probability_distribution = normalized_counts / normalized_counts_sum

        # (3) Compute the uniform distribution over the groups
        uniform_probability = 1 / probability_distribution.size

        # (4) Compute the l1 distance between the distributions.
        diff = uniform_probability - probability_distribution
        l1_distance = sum(np.abs(diff))

        # (5) Compute the total variation distance.
        tv_distance = l1_distance / 2

        return tv_distance
