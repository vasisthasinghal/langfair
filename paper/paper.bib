 % Winogender 
 % https://github.com/rudinger/winogender-schemas

@InProceedings{rudinger-EtAl:2018:N18,
  author    = {Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
  title     = {Gender Bias in Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics}
}

%WinoBias
@misc{zhao-2018,
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = {4},
	title = {{Gender Bias in Coreference Resolution: Evaluation and Debiasing methods}},
	year = {2018},
	url = {https://arxiv.org/abs/1804.06876},
}

%WinoBias+
@misc{vnmssnhv-no-date,
	author = {Vnmssnhv},
	title = {{GitHub - vnmssnhv/NeuTralRewriter: Neutral rewriter}},
	url = {https://github.com/vnmssnhv/NeuTralRewriter},
}


%GAP
%https://github.com/google-research-datasets/gap-coreference

@inproceedings{webster2018gap,
  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguou},
  author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},
  booktitle = {Transactions of the ACL},
  year =      {2018},
  pages =     {to appear},
}

%BUG
%https://github.com/SLAB-NLP/BUG

@misc{levy2021collecting,
      title={Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation}, 
      author={Shahar Levy and Koren Lazar and Gabriel Stanovsky},
      year={2021},
      eprint={2109.03858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


%StereoSet
%https://github.com/moinnadeem/stereoset, https://github.com/McGill-NLP/bias-bench

@misc{nadeem2020stereoset,
    title={StereoSet: Measuring stereotypical bias in pretrained language models},
    author={Moin Nadeem and Anna Bethke and Siva Reddy},
    year={2020},
    eprint={2004.09456},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


%BEC-PRO
%https://github.com/marionbartl/gender-bias-BERT

@inproceedings{bartl2020unmasking,
  title={Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias},
  author={Bartl, Marion and Nissim, Malvina and Gatt, Albert},
  editor={Costa-jussà, Marta R. and Hardmeier, Christian and Webster, Kellie and Radford, Will},
  booktitle={Proceedings of the Second Workshop on Gender Bias in Natural Language Processing},
  year={2020}
}

%Crows-Pairs
@inproceedings{nangia2020crows,
    title = "{CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models}",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}
%WinoQueer
@misc{katyfelkner-no-date,
	author = {Katyfelkner},
	title = {{GitHub - katyfelkner/winoqueer}},
	url = {https://github.com/katyfelkner/winoqueer},
}
%RedditBias
@misc{umanlp-no-date,
	author = {Umanlp},
	title = {{GitHub - umanlp/RedditBias: Code \& Data for the paper "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models"}},
	url = {https://github.com/umanlp/RedditBias},
}

%PANDA
@article{smith2022imsorry,
  doi = {10.48550/ARXIV.2205.09209},
  url = {https://arxiv.org/abs/2205.09209},
  author = {Smith, Eric Michael and Hall, Melissa and Kambadur, Melanie and Presani, Eleonora and Williams, Adina},
  keywords = {Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {"I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


%EquityEvaluationCorpus
@misc{unknown-author-no-date,
	title = {{Saif | Bias EEC}},
	url = {http://saifmohammad.com/WebPages/Biases-SA.html},
}


%Bias NLI
%https://github.com/sunipa/On-Measuring-and-Mitigating-Biased-Inferences-of-Word-Embeddings

@misc{dev2019measuring, title={On Measuring and Mitigating Biased Inferences of Word Embeddings}, author={Sunipa Dev and Tao Li and Jeff Phillips and Vivek Srikumar}, year={2019}, eprint={1908.09369}, archivePrefix={arXiv}, primaryClass={cs.CL} }

RealToxicityPrompts
https://toxicdegeneration.allenai.org
@inproceedings{Gehman2020RealToxicityPromptsEN,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
  booktitle={Findings},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221878771}
}

%BOLD
%https://github.com/amazon-science/bold

@inproceedings{bold_2021,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {natural language generation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

%TrustGPT
@misc{howiehwong-no-date,
	author = {HowieHwong},
	title = {{GitHub - HowieHwong/TrustGPT: Can we Trust Large Language Models?: A benchmark for responsible large language models via toxicity, Bias, and Value-alignment evaluation}},
	url = {https://github.com/HowieHwong/TrustGPT},
}


%HONEST
%https://github.com/MilaNLProc/honest

@inproceedings{nozza-etal-2021-honest,
    title = {"{HONEST}: Measuring Hurtful Sentence Completion in Language Models"},
    author = "Nozza, Debora and Bianchi, Federico  and Hovy, Dirk",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.191",
    doi = "10.18653/v1/2021.naacl-main.191",
    pages = "2398--2406",
}

@inproceedings{nozza-etal-2022-measuring,
    title = {Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals},
    author = "Nozza, Debora and Bianchi, Federico and Lauscher, Anne and Hovy, Dirk",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    publisher = "Association for Computational Linguistics",
    year={2022}
}


%BBQ
@misc{nyu-mll-no-date,
	author = {Nyu-Mll},
	title = {{GitHub - nyu-mll/BBQ: Repository for the Bias Benchmark for QA dataset.}},
	url = {https://github.com/nyu-mll/BBQ},
}


%UnQOVER
@inproceedings{li2020unqover,
      author    = {Li, Tao and Khot, Tushar and Khashabi, Daniel and Sabharwal, Ashish and Srikumar, Vivek},
      title     = {{U}n{Q}overing Stereotyping Biases via Underspecified Questions},
      booktitle = {Findings of EMNLP},
      year      = {2020}
  }


%Grep_BiasIR
%https://github.com/KlaraKrieg/GrepBiasIR

@inproceedings{krieg2022grep,
  title={Grep-BiasIR: a dataset for investigating gender representation-bias in information retrieval results},
  author={Krieg, Klara and Parada-Cabaleiro, Emilia and Medicus, Gertraud and Lesota, Oleg and Schedl, Markus and Rekabsaz, Navid},
  booktitle={Proceeding of the 2023 ACM SIGIR Conference On Human Information Interaction And Retrieval (CHIIR)},
  year={2022}
}



%LLM-FAIRNESS

%helm
@misc{liang2023holisticevaluationlanguagemodels,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09110}, 
}

% decodingtrust
@article{wang2023decodingtrust,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2023}
}

%evaluate
@misc{huggingface-no-date,
	author = {Huggingface},
	title = {GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.},
	url = {https://github.com/huggingface/evaluate},
}


%langtest
@article{Arshaan_Nazir_and_Thadaka_Kalyan_Chakravarthy_and_David_Amore_Cecchini_and_Thadaka_Kalyan_Chakravarthy_and_Rakshit_Khajuria_and_Prikshit_Sharma_and_Ali_Tarik_Mirik_and_Veysel_Kocaman_and_David_Talby_LangTest_A_comprehensive_2024,
author = {Arshaan Nazir and Thadaka Kalyan Chakravarthy and David Amore Cecchini and Thadaka Kalyan Chakravarthy and Rakshit Khajuria and Prikshit Sharma and Ali Tarik Mirik and Veysel Kocaman and David Talby},
doi = {10.1016/j.simpa.2024.100619},
journal = {Software Impacts},
number = {100619},
title = {{LangTest: A comprehensive evaluation library for custom LLM and NLP models}},
volume = {19},
year = {2024}
}

%BIG-bench
@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}
%lm-evaluation-harness
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

%TrustLLM
@inproceedings{huang2024trustllm,
  title={TrustLLM: Trustworthiness in Large Language Models},
  author={Yue Huang and Lichao Sun and Haoran Wang and Siyuan Wu and Qihui Zhang and Yuan Li and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Hanchi Sun and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bertie Vidgen and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric P. Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and Joaquin Vanschoren and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and William Yang Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=bWUU0LwwMp}
}

%lighteval
@misc{lighteval,
  author = {Fourrier, Clémentine and Habib, Nathan and Wolf, Thomas and Tunstall, Lewis},
  title = {LightEval: A lightweight framework for LLM evaluation},
  year = {2023},
  version = {0.3.0},
  url = {https://github.com/huggingface/lighteval}
}

%artkit
@misc{bcg-x-official-no-date,
	author = {Bcg-X-Official},
	title = {{GitHub - BCG-X-Official/artkit: Automated prompt-based testing and evaluation of Gen AI applications}},
	url = {https://github.com/BCG-X-Official/artkit},
}
% deepeval
@misc{confident-ai-no-date,
	author = {Confident-Ai},
	title = {{GitHub - confident-ai/deepeval: The LLM Evaluation Framework}},
	url = {https://github.com/confident-ai/deepeval},
}

%giskard
@misc{giskard-ai-no-date,
	author = {Giskard-Ai},
	title = {GitHub - Giskard-AI/giskard: Open-Source Evaluation \& Testing for ML models \& LLMs},
	url = {https://github.com/Giskard-AI/giskard},
}



%ML Fairness 
%AIF360
@misc{aif360-oct-2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}


%fairlearn
@article{Weerts_Fairlearn_Assessing_and_2023,
author = {Weerts, Hilde and Dudík, Miroslav and Edgar, Richard and Jalali, Adrin and Lutz, Roman and Madaio, Michael},
journal = {Journal of Machine Learning Research},
title = {{Fairlearn: Assessing and Improving Fairness of AI Systems}},
url = {http://jmlr.org/papers/v24/23-0389.html},
volume = {24},
year = {2023}
}

%aequitas
  @article{2018aequitas,
     title={Aequitas: A Bias and Fairness Audit Toolkit},
     author={Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid}, journal={arXiv preprint arXiv:1811.05577}, year={2018}
}

% What-if-tool
@article{DBLP:journals/corr/abs-1907-04135,
  author       = {James Wexler and
                  Mahima Pushkarna and
                  Tolga Bolukbasi and
                  Martin Wattenberg and
                  Fernanda B. Vi{\'{e}}gas and
                  Jimbo Wilson},
  title        = {The What-If Tool: Interactive Probing of Machine Learning Models},
  journal      = {CoRR},
  volume       = {abs/1907.04135},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.04135},
  eprinttype    = {arXiv},
  eprint       = {1907.04135},
  timestamp    = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-04135.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%fairness-indicators
@misc{tensorflow-no-date,
	author = {Tensorflow},
	title = {{GitHub - tensorflow/fairness-indicators: Tensorflow's Fairness Evaluation and Visualization Toolkit}},
	url = {https://github.com/tensorflow/fairness-indicators},
}


% DALEX
@article{Biecek-2018-DALEX-Explainers-for-Complex-Predictive-Models-in-R,
	author = {Biecek, P.},
	date-modified = {2023-04-19 09:31:51 -0400},
	journal = {Journal of Machine Learning Research},
	number = {84},
	pages = {1-5},
	title = {DALEX: Explainers for Complex Predictive Models in R},
	url = {https://jmlr.org/papers/v19/18-416.html},
	volume = {19},
	year = {2018},
	bdsk-url-1 = {https://jmlr.org/papers/v19/18-416.html}}


%LiFT
@inproceedings{vasudevan20lift,
    author       = {Vasudevan, Sriram and Kenthapadi, Krishnaram},
    title        = {{LiFT}: A Scalable Framework for Measuring Fairness in ML Applications},
    booktitle    = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
    series       = {CIKM '20},
    year         = {2020},
    pages        = {},
    numpages     = {8}
}

@misc{lift,
    author       = {Vasudevan, Sriram and Kenthapadi, Krishnaram},
    title        = {The LinkedIn Fairness Toolkit ({LiFT})},
    howpublished = {\url{https://github.com/linkedin/lift}},
    month        = aug,
    year         = 2020
}


@misc{bouchard2024actionableframeworkassessingbias,
      title={An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases}, 
      author={Dylan Bouchard},
      year={2024},
      eprint={2407.10853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10853}, 
}

% surveys
@misc{minaee2024large,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{generalsurvey,
title = {ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {3},
pages = {121-154},
year = {2023},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266734522300024X},
author = {Partha Pratim Ray},
keywords = {ChatGPT, Language model, GPT-3.5, Generative AI, Conversational AI, Context understanding, Natural language processing},
}


@article{chatgpt_survey,
	doi = {10.1016/j.metrad.2023.100017},
  
	url = {https://doi.org/10.1016%2Fj.metrad.2023.100017},
  
	year = 2023,
	month = {sep},
  
	publisher = {Elsevier {BV}
},
  
	volume = {1},
  
	number = {2},
  
	pages = {100017},
  
	author = {Yiheng Liu and Tianle Han and Siyuan Ma and Jiayue Zhang and Yuanyuan Yang and Jiaming Tian and Hao He and Antong Li and Mengshen He and Zhengliang Liu and Zihao Wu and Lin Zhao and Dajiang Zhu and Xiang Li and Ning Qiang and Dingang Shen and Tianming Liu and Bao Ge},
  
	title = {Summary of {ChatGPT}-Related research and perspective towards the future of large language models},
  
	journal = {Meta-Radiology}
}

%COBS
@inproceedings{bordia-bowman-2019-identifying,
    title = "Identifying and Reducing Gender Bias in Word-Level Language Models",
    author = "Bordia, Shikha  and
      Bowman, Samuel R.",
    editor = "Kar, Sudipta  and
      Nadeem, Farah  and
      Burdick, Laura  and
      Durrett, Greg  and
      Han, Na-Rae",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-3002",
    doi = "10.18653/v1/N19-3002",
    pages = "7--15",
    abstract = "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.",
}

%stereotype classifiers
@misc{zekun2023auditinglargelanguagemodels,
      title={Towards Auditing Large Language Models: Improving Text-based Stereotype Detection}, 
      author={Wu Zekun and Sahan Bulathwela and Adriano Soares Koshiyama},
      year={2023},
      eprint={2311.14126},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.14126}, 
}


% counterfactual fairness
@misc{kusner2018counterfactualfairness,
      title={Counterfactual Fairness}, 
      author={Matt J. Kusner and Joshua R. Loftus and Chris Russell and Ricardo Silva},
      year={2018},
      eprint={1703.06856},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.06856}, 
}

% Counterfactual fairness 2 
@inproceedings{10.1145/3351095.3372851,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual risk assessments, evaluation, and fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}

% CF 3

@article{osti_10126321,
place = {Country unknown/Code not available}, title = {Counterfactual Fairness: Unidentification, Bound and Algorithm}, url = {https://par.nsf.gov/biblio/10126321}, DOI = {10.24963/ijcai.2019/199}, abstractNote = {Fairness-aware learning studies the problem of building machine learning models that are subject to fairness requirements. Counterfactual fairness is a notion of fairness derived from Pearl's causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the counterfactual world where the individual(s) had belonged to a different demographic group. However, an inherent limitation of counterfactual fairness is that it cannot be uniquely quantified from the observational data in certain situations, due to the unidentifiability of the counterfactual quantity. In this paper, we address this limitation by mathematically bounding the unidentifiable counterfactual quantity, and develop a theoretically sound algorithm for constructing counterfactually fair classifiers. We evaluate our method in the experiments using both synthetic and real-world datasets, as well as compare with existing methods. The results validate our theory and show the effectiveness of our method.}, journal = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence}, author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao}, }

% Cf 4
@article{Rosenblatt_Witter_2023, title={Counterfactual Fairness Is Basically Demographic Parity}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26691}, DOI={10.1609/aaai.v37i12.26691}, abstractNote={Making fair decisions is crucial to ethically implementing machine learning algorithms in social settings. In this work, we consider the celebrated definition of counterfactual fairness. We begin by showing that an algorithm which satisfies counterfactual fairness also satisfies demographic parity, a far simpler fairness constraint. Similarly, we show that all algorithms satisfying demographic parity can be trivially modified to satisfy counterfactual fairness. Together, our results indicate that counterfactual fairness is basically equivalent to demographic parity, which has important implications for the growing body of work on counterfactual fairness. We then validate our theoretical findings empirically, analyzing three existing algorithms for counterfactual fairness against three simple benchmarks. We find that two simple benchmark algorithms outperform all three existing algorithms---in terms of fairness, accuracy, and efficiency---on several data sets. Our analysis leads us to formalize a concrete fairness goal: to preserve the order of individuals within protected groups. We believe transparency around the ordering of individuals within protected groups makes fair algorithms more trustworthy. By design, the two simple benchmark algorithms satisfy this goal while the existing algorithms do not.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Rosenblatt, Lucas and Witter, R. Teal}, year={2023}, month={Jun.}, pages={14461-14469} }


% gallegos survey
@misc{gallegos2024biasfairnesslargelanguage,
      title={Bias and Fairness in Large Language Models: A Survey}, 
      author={Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Sungchul Kim and Franck Dernoncourt and Tong Yu and Ruiyi Zhang and Nesreen K. Ahmed},
      year={2024},
      eprint={2309.00770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00770}, 
}


% counterfactual sentiment
@misc{huang2020reducingsentimentbiaslanguage,
      title={Reducing Sentiment Bias in Language Models via Counterfactual Evaluation}, 
      author={Po-Sen Huang and Huan Zhang and Ray Jiang and Robert Stanforth and Johannes Welbl and Jack Rae and Vishal Maini and Dani Yogatama and Pushmeet Kohli},
      year={2020},
      eprint={1911.03064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.03064}, 
}


% rouge 
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}


% BLEU 
@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}


% Rec

@inproceedings{Zhang_2023, series={RecSys ’23},
   title={Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation},
   volume={2012},
   url={http://dx.doi.org/10.1145/3604915.3608860},
   DOI={10.1145/3604915.3608860},
   booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
   publisher={ACM},
   author={Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
   year={2023},
   month=sep, pages={993–999},
   collection={RecSys ’23} }

% DI
@misc{feldman2015certifyingremovingdisparateimpact,
      title={Certifying and removing disparate impact}, 
      author={Michael Feldman and Sorelle Friedler and John Moeller and Carlos Scheidegger and Suresh Venkatasubramanian},
      year={2015},
      eprint={1412.3756},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1412.3756}, 
}



BibTeX
@inproceedings{10.5555/3120007.3120011,
author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
title = {Fairness-aware classifier with prejudice remover regularizer},
year = {2012},
isbn = {9783642334856},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
booktitle = {Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II},
pages = {35–50},
numpages = {16},
keywords = {classification, discrimination, fairness, information theory, logistic regression, social responsibility},
location = {Bristol, UK},
series = {ECMLPKDD'12}
}

% EOP
@misc{hardt2016equalityopportunitysupervisedlearning,
      title={Equality of Opportunity in Supervised Learning}, 
      author={Moritz Hardt and Eric Price and Nathan Srebro},
      year={2016},
      eprint={1610.02413},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1610.02413}, 
}

%CEOP
@misc{pleiss2017fairnesscalibration,
      title={On Fairness and Calibration}, 
      author={Geoff Pleiss and Manish Raghavan and Felix Wu and Jon Kleinberg and Kilian Q. Weinberger},
      year={2017},
      eprint={1709.02012},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1709.02012}, 
}

%ROCP
@inproceedings{10.1109/ICDM.2012.45,
author = {Kamiran, Faisal and Karim, Asim and Zhang, Xiangliang},
title = {Decision Theory for Discrimination-Aware Classification},
year = {2012},
isbn = {9780769549057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDM.2012.45},
doi = {10.1109/ICDM.2012.45},
abstract = {Social discrimination (e.g., against females) arising from data mining techniques is a growing concern worldwide. In recent years, several methods have been proposed for making classifiers learned over discriminatory data discrimination-aware. However, these methods suffer from two major shortcomings: (1) They require either modifying the discriminatory data or tweaking a specific classification algorithm and (2) They are not flexible w.r.t. discrimination control and multiple sensitive attribute handling. In this paper, we present two solutions for discrimination-aware classification that neither require data modification nor classifier tweaking. Our first and second solutions exploit, respectively, the reject option of probabilistic classifier(s) and the disagreement region of general classifier ensembles to reduce discrimination. We relate both solutions with decision theory for better understanding of the process. Our experiments using real-world datasets demonstrate that our solutions outperform existing state-of-the-art methods, especially at low discrimination which is a significant advantage. The superior performance coupled with flexible control over discrimination and easy applicability to multiple sensitive attributes makes our solutions an important step forward in practical discrimination-aware classification.},
booktitle = {Proceedings of the 2012 IEEE 12th International Conference on Data Mining},
pages = {924–929},
numpages = {6},
keywords = {classification, decision theory, ensembles, social discrimination},
series = {ICDM '12}
}

% reductions
@misc{agarwal2018reductionsapproachfairclassification,
      title={A Reductions Approach to Fair Classification}, 
      author={Alekh Agarwal and Alina Beygelzimer and Miroslav Dudík and John Langford and Hanna Wallach},
      year={2018},
      eprint={1803.02453},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.02453}, 
}

% Adversarial debiasing
@misc{zhang2018mitigatingunwantedbiasesadversarial,
      title={Mitigating Unwanted Biases with Adversarial Learning}, 
      author={Brian Hu Zhang and Blake Lemoine and Margaret Mitchell},
      year={2018},
      eprint={1801.07593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.07593}, 
}



@article{intrinsic_do_not,
  author       = {Seraphina Goldfarb{-}Tarrant and
                  Rebecca Marchant and
                  Ricardo Mu{\~{n}}oz S{\'{a}}nchez and
                  Mugdha Pandya and
                  Adam Lopez},
  title        = {Intrinsic Bias Metrics Do Not Correlate with Application Bias},
  journal      = {CoRR},
  volume       = {abs/2012.15859},
  year         = {2020},
  url          = {https://arxiv.org/abs/2012.15859},
  eprinttype    = {arXiv},
  eprint       = {2012.15859},
  timestamp    = {Thu, 14 Oct 2021 09:15:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-15859.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{biased_rulers,
  author       = {Pieter Delobelle and
                  Ewoenam Kwaku Tokpo and
                  Toon Calders and
                  Bettina Berendt},
  title        = {Measuring Fairness with Biased Rulers: {A} Survey on Quantifying Biases
                  in Pretrained Language Models},
  journal      = {CoRR},
  volume       = {abs/2112.07447},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.07447},
  eprinttype    = {arXiv},
  eprint       = {2112.07447},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-07447.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
